{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12556c-d64e-400f-8a6a-e1df2cca0bbf",
   "metadata": {},
   "source": [
    "# Exploring Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab7947-8d09-4392-92d3-bfd182eb5b89",
   "metadata": {},
   "source": [
    "## Loading the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6363c5c-01b6-4e99-a259-e24dc291719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"LTIMindtree Q2FY24: Show of strength. Good revenue growth and resilient margin performance\",\n",
    "        \"The company expects furloughs to be more pronounced in Q3 and it is guiding to a very weak quarter, with revenue decline between 1.5 percent and 3.5 percent\",\n",
    "        \"Arkam Ventures is also an investor in Jai Kisan, one of India’s fastest-growing rural fintech platforms for farmers and retailers, and Jumbotail, India’s leading B2B food and grocery marketplace and retail platform\",\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f573d-88e3-4255-8479-eb3475c4a391",
   "metadata": {},
   "source": [
    "## Loading Tokenizers\n",
    "\n",
    "Loading tokenizer can be done using `from_pretrained()` method of any Tokenizer class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c90a71-7b2b-46c3-ad44-7d418095460a",
   "metadata": {},
   "source": [
    "### AutoTokenizer class \n",
    "This will load the specific tokenizer based on the input provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba77cf82-73b7-4fb4-83f6-f6a076ce4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6009bb7-b86f-49ee-a502-b1175d81ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb1411c-17f3-4814-a114-eda2ad1b890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8318, 27605, 26379, 9910, 1053, 2475, 12031, 18827, 1024, 2265, 1997, 3997, 1012, 2204, 6599, 3930, 1998, 24501, 18622, 4765, 7785, 2836, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1858b-a1f2-4655-a22f-61828a9e7275",
   "metadata": {},
   "source": [
    "### [ModelName]Tokenizer Class \n",
    "This will load the model specific tokenizer from the specified checkpoint.\n",
    "\n",
    "   For Example,  `DistibertTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642909e3-6053-4ad9-bafe-948d59a0bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec9787c-5830-48e2-9665-78bda7944a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4833a7cb-0f40-45de-ada6-823af9126a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8318, 27605, 26379, 9910, 1053, 2475, 12031, 18827, 1024, 2265, 1997, 3997, 1012, 2204, 6599, 3930, 1998, 24501, 18622, 4765, 7785, 2836, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_tokenizer(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de606ab-ebd0-44cd-aa7b-501225629209",
   "metadata": {},
   "source": [
    "## Saving the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc9ed47-2dc9-40a6-ac46-28a7bf36696c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./../../../hf_models/distilbert-base-uncased/tokenizer_config.json',\n",
       " './../../../hf_models/distilbert-base-uncased/special_tokens_map.json',\n",
       " './../../../hf_models/distilbert-base-uncased/vocab.txt',\n",
       " './../../../hf_models/distilbert-base-uncased/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_tokenizer.save_pretrained('./../../../hf_models/distilbert-base-uncased/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529b6ea-5158-417d-9304-8ed4e9070ea5",
   "metadata": {},
   "source": [
    "## Tokenization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b02c18c-4f08-4c60-88b2-5e95168b2dc3",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Breaking the input into tokens specific to the `vocab` of the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af68731-95bc-40e3-99c1-8add82dadc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lt', '##imi', '##ndt', '##ree', 'q', '##2', '##fy', '##24', ':', 'show', 'of', 'strength', '.', 'good', 'revenue', 'growth', 'and', 'res', '##ili', '##ent', 'margin', 'performance']\n"
     ]
    }
   ],
   "source": [
    "tokens = distilbert_tokenizer.tokenize(data[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f00422-e98d-43cd-a42a-387f0de02339",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Mapping tokens to input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fc43b8-2529-42c2-9556-2781bcff5959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8318, 27605, 26379, 9910, 1053, 2475, 12031, 18827, 1024, 2265, 1997, 3997, 1012, 2204, 6599, 3930, 1998, 24501, 18622, 4765, 7785, 2836]\n"
     ]
    }
   ],
   "source": [
    "ids = distilbert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018d280-54ad-4c27-94df-68a314bbe589",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "Mapping input IDs back to tokens and grouping them back to the same words as in input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee1016f-be63-4ced-b017-996a11c191e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltimindtree q2fy24 : show of strength. good revenue growth and resilient margin performance\n"
     ]
    }
   ],
   "source": [
    "print(distilbert_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03102091-c4c4-4f1a-86f4-f917b1049f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] ltimindtree q2fy24 : show of strength. good revenue growth and resilient margin performance [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(distilbert_tokenizer.decode(distilbert_tokenizer(data[0])['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f993c-1466-4241-9cc1-3ccfc7a8f813",
   "metadata": {},
   "source": [
    "## Handling Batches\n",
    "\n",
    "1. Model accepts input in batches, i.e. multiple sentences, all at once.\n",
    "2. In a dataset of n sentences, sentences could be of varying lengths. Hence, they needed to be padded or truncated, so that every sentence in the batch is of the same length.\n",
    "3. Every tokenizer has some special tokens such as `[CLS]`, `[SEP]`, etc. One of them is `[PAD]` token as well.\n",
    "4. *Attention masks* are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (such as `[PAD]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87154a0d-1a00-41cd-b89a-d70bc51d95cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 8318, 27605, 26379, 9910, 1053, 2475, 12031, 18827, 1024, 2265, 1997, 3997, 1012, 2204, 6599, 3930, 1998, 24501, 18622, 4765, 7785, 2836, 102], [101, 1996, 2194, 24273, 6519, 23743, 5603, 2015, 2000, 2022, 2062, 8793, 1999, 1053, 2509, 1998, 2009, 2003, 14669, 2000, 1037, 2200, 5410, 4284, 1010, 2007, 6599, 6689, 2090, 1015, 1012, 1019, 3867, 1998, 1017, 1012, 1019, 3867, 102], [101, 15745, 3286, 13252, 2003, 2036, 2019, 14316, 1999, 17410, 11382, 8791, 1010, 2028, 1997, 2634, 1521, 1055, 7915, 1011, 3652, 3541, 10346, 15007, 7248, 2005, 6617, 1998, 16629, 1010, 1998, 18414, 13344, 14162, 1010, 2634, 1521, 1055, 2877, 1038, 2475, 2497, 2833, 1998, 13025, 18086, 1998, 7027, 4132, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = distilbert_tokenizer(data)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639434f1-b863-48d4-8a29-c090454be2f4",
   "metadata": {},
   "source": [
    "##### Padding with model's max_length parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad327199-271a-4d70-a3e6-131f56e9595c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 8318, 27605, 26379, 9910, 1053, 2475, 12031, 18827, 1024, 2265, 1997, 3997, 1012, 2204, 6599, 3930, 1998, 24501, 18622, 4765, 7785, 2836, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 2194, 24273, 6519, 23743, 5603, 2015, 2000, 2022, 2062, 8793, 1999, 1053, 2509, 1998, 2009, 2003, 14669, 2000, 1037, 2200, 5410, 4284, 1010, 2007, 6599, 6689, 2090, 1015, 1012, 1019, 3867, 1998, 1017, 1012, 1019, 3867, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 15745, 3286, 13252, 2003, 2036, 2019, 14316, 1999, 17410, 11382, 8791, 1010, 2028, 1997, 2634, 1521, 1055, 7915, 1011, 3652, 3541, 10346, 15007, 7248, 2005, 6617, 1998, 16629, 1010, 1998, 18414, 13344, 14162, 1010, 2634, 1521, 1055, 2877, 1038, 2475, 2497, 2833, 1998, 13025, 18086, 1998, 7027, 4132, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = distilbert_tokenizer(data, padding=\"max_length\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11146e9-4cab-4371-b296-a8443648f75c",
   "metadata": {},
   "source": [
    "##### Padding with length of longest sequence in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76cb7787-2b5c-41aa-984d-7c424674aad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 8318, 27605, 26379, 9910, 1053, 2475, 12031, 18827, 1024, 2265, 1997, 3997, 1012, 2204, 6599, 3930, 1998, 24501, 18622, 4765, 7785, 2836, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 2194, 24273, 6519, 23743, 5603, 2015, 2000, 2022, 2062, 8793, 1999, 1053, 2509, 1998, 2009, 2003, 14669, 2000, 1037, 2200, 5410, 4284, 1010, 2007, 6599, 6689, 2090, 1015, 1012, 1019, 3867, 1998, 1017, 1012, 1019, 3867, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 15745, 3286, 13252, 2003, 2036, 2019, 14316, 1999, 17410, 11382, 8791, 1010, 2028, 1997, 2634, 1521, 1055, 7915, 1011, 3652, 3541, 10346, 15007, 7248, 2005, 6617, 1998, 16629, 1010, 1998, 18414, 13344, 14162, 1010, 2634, 1521, 1055, 2877, 1038, 2475, 2497, 2833, 1998, 13025, 18086, 1998, 7027, 4132, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = distilbert_tokenizer(data, padding=\"longest\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f6831-12bd-490e-9cc2-af87dc7e8155",
   "metadata": {},
   "source": [
    "##### Truncating the sequence that are larger than model's max_length parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29905deb-c900-4f55-990e-6767e8fb86c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 8318, 27605, 26379, 9910, 1053, 2475, 12031, 18827, 1024, 2265, 1997, 3997, 1012, 2204, 6599, 3930, 1998, 24501, 18622, 4765, 7785, 2836, 102], [101, 1996, 2194, 24273, 6519, 23743, 5603, 2015, 2000, 2022, 2062, 8793, 1999, 1053, 2509, 1998, 2009, 2003, 14669, 2000, 1037, 2200, 5410, 4284, 1010, 2007, 6599, 6689, 2090, 1015, 1012, 1019, 3867, 1998, 1017, 1012, 1019, 3867, 102], [101, 15745, 3286, 13252, 2003, 2036, 2019, 14316, 1999, 17410, 11382, 8791, 1010, 2028, 1997, 2634, 1521, 1055, 7915, 1011, 3652, 3541, 10346, 15007, 7248, 2005, 6617, 1998, 16629, 1010, 1998, 18414, 13344, 14162, 1010, 2634, 1521, 1055, 2877, 1038, 2475, 2497, 2833, 1998, 13025, 18086, 1998, 7027, 4132, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = distilbert_tokenizer(data, truncation=True)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4430f-9662-472c-8b15-e1a33ea061f7",
   "metadata": {},
   "source": [
    "##### Final input to the model\n",
    "\n",
    "1. Padding till the length of the longest sequence.\n",
    "2. Truncating all those sentences with length greater than the model's max_length.\n",
    "3. Returning PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae89b65-a94a-450b-b424-3b57e7b9118e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  8318, 27605, 26379,  9910,  1053,  2475, 12031, 18827,  1024,\n",
       "          2265,  1997,  3997,  1012,  2204,  6599,  3930,  1998, 24501, 18622,\n",
       "          4765,  7785,  2836,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1996,  2194, 24273,  6519, 23743,  5603,  2015,  2000,  2022,\n",
       "          2062,  8793,  1999,  1053,  2509,  1998,  2009,  2003, 14669,  2000,\n",
       "          1037,  2200,  5410,  4284,  1010,  2007,  6599,  6689,  2090,  1015,\n",
       "          1012,  1019,  3867,  1998,  1017,  1012,  1019,  3867,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101, 15745,  3286, 13252,  2003,  2036,  2019, 14316,  1999, 17410,\n",
       "         11382,  8791,  1010,  2028,  1997,  2634,  1521,  1055,  7915,  1011,\n",
       "          3652,  3541, 10346, 15007,  7248,  2005,  6617,  1998, 16629,  1010,\n",
       "          1998, 18414, 13344, 14162,  1010,  2634,  1521,  1055,  2877,  1038,\n",
       "          2475,  2497,  2833,  1998, 13025, 18086,  1998,  7027,  4132,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = distilbert_tokenizer(data, truncation=True, padding=True, return_tensors='pt')\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e4460-daa1-4f03-9c5f-4922bbb1ff11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

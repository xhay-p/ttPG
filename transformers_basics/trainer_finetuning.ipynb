{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c04f4a-6c46-4e7d-ac0b-4c6da3e55ea6",
   "metadata": {},
   "source": [
    "# Fine-tuning a transformer models using Trainer API\n",
    "\n",
    "[Trainer](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#trainer) class helps fine-tune any of the pre-trained models.\n",
    "\n",
    "\n",
    "**NOTE:** Trainer Class is not compatible with CPU. This code will either run very slow or throw error in CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61ff22-3bd9-4b8f-8b47-82d90466e295",
   "metadata": {},
   "source": [
    "## Processing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698ceba-0f43-48b3-b379-cdac2a94e6e7",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058bb77-21c3-4f10-8060-e201ecfd4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a8d3d-27a1-4e04-9c91-4a908167b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset('financial_phrasebank', 'sentences_allagree')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45982f-9af5-4a30-95ff-3cc9e7286af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = raw_dataset['train'].train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f53ccc-d3fe-47aa-8493-c5fcfee2e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9e242-0bcf-40c7-8878-0ff7e42f9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['train'][25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6408e8-5cd5-4654-b166-a23cff1a3c8e",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1654384-58b8-4936-a536-7d6a0c0436ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48805db-c730-40a6-b21f-69c9f3c399e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadda95a-c666-4d22-9a0e-011ca79918c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(example):\n",
    "    return tokenizer(example['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61268227-ef7e-404d-a6a3-c03fd15f73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = raw_dataset.map(tokenize_dataset, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b2742-6492-47b0-900a-833b0264f923",
   "metadata": {},
   "source": [
    "`Dataset.map()` gives flexibility to preprocess all the dataset at once efficiently without breaking the `DatasetDict` object. `The map()` method works by applying a function on each element of the dataset. Any other pre-processing can also be easily applied using the `map()`.\n",
    "\n",
    "The above function `tokenize_dataset(example)` function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys `input_ids` and `attention_mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214f84b-3bb4-4476-ba6a-0e3517055ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5d159-1869-45ea-9314-95271626cc69",
   "metadata": {},
   "source": [
    "If padding had been done earlier, then it would have padded all the sentences with the size of *longest sequence*, which is not a good practice. It is more efficient to pad while building a batch; it will pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths.\n",
    "\n",
    "This process of padding during building the batch is called *dynamic padding*. This can be achieved using `collate function` while building `DataLoader`. The collate function is passed as a parameter in the DataLoader. One can write their own collate function or can use the pre-written ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d5f43-94db-4f38-8169-47d36ab75eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCollator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb99b1-eaba-4406-8a94-133503e0362b",
   "metadata": {},
   "source": [
    "## Fine-tuning the model with Trainer api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2a42b-b26f-4d2b-aa3b-f70086dfcee5",
   "metadata": {},
   "source": [
    "### Defining the training arguements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8cefa-8c38-481a-bc73-be1a4b3ecdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109959b7-8a55-4f66-81c7-b6bbf4f36fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "                    output_dir='./../../model/fin_sentiment_distilbert/',\n",
    "                    evaluation_strategy='epoch',\n",
    "                    learning_rate=2e-5,\n",
    "                    num_train_epochs=3,\n",
    "                    weight_decay=0.01,                    \n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf1a16-ea8a-400c-986d-5fe64bdf4419",
   "metadata": {},
   "source": [
    "### Definining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27ac5d-d714-4f3a-9885-a2e4c21deddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b525d6-967b-4047-a430-f4c78b80e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7423f8a-0733-455a-b4d8-77ed29e36081",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7267bb-053d-4133-89ff-6e8d79f740e5",
   "metadata": {},
   "source": [
    "### Defining the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd414357-f25e-43c3-b5f6-83ffc6e1ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average='macro')[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128e8a7-ba56-4b6a-b670-276953cc30a6",
   "metadata": {},
   "source": [
    "### Defining the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c70790-c6f1-4eb2-9a97-6f88ccd52c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=dataCollator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edbe1f-7270-4eaa-aa87-292f7d2dadf2",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b0f56-696b-4c24-ad9e-ec98e9c6a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d7410-a84e-4fc0-ad65-bca0bed1320d",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea0fc5-7ca2-4d15-bce7-44b2a9827808",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

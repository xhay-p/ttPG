{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5c9b36-d242-4ee0-84e4-a6247942f774",
   "metadata": {},
   "source": [
    "## Model Quantization for Inference\n",
    "\n",
    "LLMs are very large in size and to efficiently load models for inferencing on real-time, low-resource settings such as single GPUs, it is required to significantly reduce the size of LLMs without losing much on performance. This process is known as *Quatization*.\n",
    "\n",
    "In this notebook, I have compiled popular quantization algorithms/strategies available through huggingface `transformers`.\n",
    "\n",
    "**NOTE:** To implement this notebook, *GPU* is required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e071a-f2bc-4f8d-b1ae-e86b74e5ce09",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a04de6",
   "metadata": {},
   "source": [
    "## Full Precision\n",
    "\n",
    "IEEE 32 bit floating point. This is `float32` or `FP32` precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d6b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, trust_remote_code=True, device_map=\"auto\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b42c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "print(f\"GPU memory: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb1053",
   "metadata": {},
   "source": [
    "* Model has simple linear layers.\n",
    "* Model weights are stored in FP32.\n",
    "* Computation dtype is also FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a91401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "import torch\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b2673",
   "metadata": {},
   "source": [
    "## Half Precision\n",
    "2 Bytes or 16 bit floating point. There are 2 variants:\n",
    "1. `FP16` is normal 16 bit floating point.\n",
    "2. `BF16` (bfloat 16) is more optimised version of floating 16 bit precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed533660",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "print(f\"GPU memory: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c0608",
   "metadata": {},
   "source": [
    "Model size just got half. From around **28 GB** in `FP32` Precision to around **14 GB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d878a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b79288",
   "metadata": {},
   "source": [
    "* Model weights are store in `BF16` precision.\n",
    "* Computation also uses `BF16` precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e08639",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f05c65",
   "metadata": {},
   "source": [
    "## INT8 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af563558",
   "metadata": {},
   "source": [
    "### Using transformers parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de0d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\",\n",
    "            load_in_8bit=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0449e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "print(f\"GPU memory: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e2a95",
   "metadata": {},
   "source": [
    "* Model size got further reduced by half **7.5 GB**.\n",
    "* `8bit` Precision is used for just storing the model.\n",
    "* Computation uses `BF16` precision as mentioned in the parameter `torch_dtype`. This can take `FP32` or `FP16` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec318989",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ac824",
   "metadata": {},
   "source": [
    "Simple Linear is replaced with  **Linear8bitLt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93638b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66f370",
   "metadata": {},
   "source": [
    "Model `dtype` shows `torch.bfloat16`: This means the model will store weigths with **8bit**. While the computation will happen in `torch.bfloat16`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0423d",
   "metadata": {},
   "source": [
    "### Using bitsandbytes quantization config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecbf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True,\n",
    "                                         llm_int8_threshold=200.0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0502418",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "print(f\"GPU memory: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903641c9",
   "metadata": {},
   "source": [
    "## 4bit Model Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e990ad",
   "metadata": {},
   "source": [
    "### Using transformers parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23948e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\",\n",
    "            load_in_4bit=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a93318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "print(f\"GPU memory: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a020bb",
   "metadata": {},
   "source": [
    "* Model `dtype` shows `torch.bfloat16`: This means the model will store weigths with **4bit**. While the computation will happen in `torch.bfloat16`.\n",
    "* Model size further reduced to **4.24 GB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e4b48",
   "metadata": {},
   "source": [
    "Simple Linear is replaced with  **Linear4bit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77feb8",
   "metadata": {},
   "source": [
    "### using bitsandbytes quantization config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e82993",
   "metadata": {},
   "source": [
    "#### QLoRA NF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, trust_remote_code=True, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "print(f\"GPU memory: {model.get_memory_footprint() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory: {model.get_memory_footprint():.2f} B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31902774",
   "metadata": {},
   "source": [
    "#### QLoRA NF4-double-quantization (Nested Quantization)\n",
    "\n",
    "Using `bnb_4bit_use_double_quant` argument, double or nested quantization can be achieved. This will enable second quantization after the first one to save additional 0.4 bits per parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, trust_remote_code=True, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "print(f\"GPU memory: {model.get_memory_footprint() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory: {model.get_memory_footprint():.2f} B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afac12c-8bb2-425f-b270-e9d1e10cd4c1",
   "metadata": {},
   "source": [
    "Here, the impact of double quantization is not clearly evident. However, for larger models, there is an observable difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bdfa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5ff75",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e16b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcb90b",
   "metadata": {},
   "source": [
    "The GPTQ algorithm requires to callibrate the quantized weights of the model by doing inference on the quantized model. For quantizing a model using auto-gptq, it is required to pass a dataset to quantizer. Default supported datasets are `['wikitext2', 'c4', 'c4-new', 'ptb', 'ptb-new']`. One can also pass their own dataset as a `list of strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set quantization configuration\n",
    "quantization_config = GPTQConfig(\n",
    "     bits=4,\n",
    "     group_size=128,\n",
    "     dataset=\"c4\",\n",
    "     desc_act=False,\n",
    "     tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37d91c-7b1c-4aa5-9432-1ae1795537d7",
   "metadata": {},
   "source": [
    "### Loading the model and quantizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from HF\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                quantization_config=quantization_config, trust_remote_code=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227c7f0",
   "metadata": {},
   "source": [
    "This process takes few hours to generate a quantized model. Hence, it is recommended to save the model locally or push it to huggingface_hub for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000d806-b29e-4b2d-8375-bc72b7bfe3a1",
   "metadata": {},
   "source": [
    "### Saving the quantized model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24166003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the quantize model to disk\n",
    "save_folder = \"./models/quantized-shearedllama-2.7b\"\n",
    "quant_model.save_pretrained(save_folder, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "tokenizer.save_pretrained(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de702222",
   "metadata": {},
   "outputs": [],
   "source": [
    "del quant_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f624f",
   "metadata": {},
   "source": [
    "### Loading the Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94467f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/quantized-shearedllama-2.7b/\"\n",
    "gptq_config = GPTQConfig(bits=4, disable_exllama=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", \n",
    "                                             quantization_config = gptq_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebb463",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "print(f\"GPU memory: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e12034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c5ce2",
   "metadata": {},
   "source": [
    "Linear layer will be modified by by **QuantLinear** layer from auto-gptq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config.quantization_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8939ffa",
   "metadata": {},
   "source": [
    "## Imprtant References\n",
    "* HuggingFace Blogs\n",
    "    1. https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "    2. https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "    3. https://huggingface.co/blog/gptq-integration\n",
    "    4. https://huggingface.co/blog/merve/quantization\n",
    "    5. https://huggingface.co/blog/overview-quantization-transformers\n",
    "    6. https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/quantization\n",
    "* Research Papers\n",
    "    1. [LLM.int8](https://arxiv.org/abs/2208.07339)\n",
    "    2. [QLoRA](https://arxiv.org/abs/2305.14314)\n",
    "    3. [GPTQ](https://arxiv.org/pdf/2210.17323.pdf)\n",
    "* Github Repo\n",
    "    1. [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)\n",
    "    2. [auto-gptq](https://github.com/PanQiWei/AutoGPTQ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c28a7cf-8431-45c6-a1e2-81fc058cfa83",
   "metadata": {},
   "source": [
    "# Exploing `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30441e-f767-498d-8b85-47f7f49ed7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddece1-e6e4-4d3c-8bd2-386073888596",
   "metadata": {},
   "source": [
    "### Loading a local dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40854b5c-d89e-46c1-b11a-aecad741bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_dataset = load_dataset(\"csv\", data_files=\"./../../data/FiQA_and_Financial_PhraseBank_in_1/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3da30c-8b23-4762-9029-ecd7b36d303e",
   "metadata": {},
   "source": [
    "|Data format       |Loading script| \tExample                                              |\n",
    "|------------------|:------------:|---------------------------------------------------------:|\n",
    "|CSV & TSV         |csv           | \tload_dataset(\"csv\", data_files=\"my_file.csv\")        |\n",
    "|Text files        |text          | \tload_dataset(\"text\", data_files=\"my_file.txt\")       |\n",
    "|JSON & JSON Lines |json          | \tload_dataset(\"json\", data_files=\"my_file.jsonl\")     |\n",
    "|Pickled DataFrames|pandas        | \tload_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5f8b5-d05c-4729-80ad-e686aad7c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d75e7e-af8e-4596-b1ca-d333e499dbc1",
   "metadata": {},
   "source": [
    "This creates `DatasetDict` object with a train split. If there are multiple files such as train, dev, and test, the `data_files` argument of the `load_dataset()` function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc172c1-311b-4188-b1ae-7c670b2ba3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'train':'./../../data/re-tacred/train.json', \n",
    "              'dev':'./../../data/re-tacred/dev.json', \n",
    "              'test':'./../../data/re-tacred/test.json'}\n",
    "\n",
    "re_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "re_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95439e-e634-4ec4-bc29-ff6f9f0f8ebe",
   "metadata": {},
   "source": [
    "**NOTE:** `load_dataset() fucntion can also perform automatic decompression to common formats like ZIP and TAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61f423-681c-44b5-b842-e2d5d5f7941a",
   "metadata": {},
   "source": [
    "## Dataset Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55013ef8-b653-4616-8dad-0046e4aec324",
   "metadata": {},
   "source": [
    "##### Selecting a random sample for data analysis.\n",
    "\n",
    "`Dataset.select()` expects an iterable of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57590779-c5d9-471e-a613-40e7108922ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sc_dataset[\"train\"].shuffle(seed=25).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80cc97a-4682-4c7b-af8b-ef55ff1175c7",
   "metadata": {},
   "source": [
    "### Dataset Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599478c-8cc4-437c-b733-1cf59f9e4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1eea1-68f5-4bee-84d3-5824abd9360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf7e26-cb44-44db-8ddd-3e0ac9a369cc",
   "metadata": {},
   "source": [
    "### Important functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d993d71-3f6a-4111-81b2-5958b3681fe2",
   "metadata": {},
   "source": [
    "##### unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce72e9c-6e3a-40b1-9b37-9f3c8ed15af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.unique('Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e5cf7-3b9b-4ab9-8855-7f06f23c01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in re_dataset.keys():\n",
    "    assert len(re_dataset[split].unique('id')) == len(re_dataset[split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949749b2-f180-4008-804c-7ee02787ee32",
   "metadata": {},
   "source": [
    "##### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d81d4-67ac-4463-aafe-cf339b4973ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering samples with sentence length greater than 5\n",
    "sample = sample.filter(lambda x: len(x[\"Sentence\"].split()) > 5)\n",
    "print(len(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d3b64-9645-44e0-9416-444b900fa4e4",
   "metadata": {},
   "source": [
    "##### map()\n",
    "\n",
    "The `map()` function supports processing batches of examples at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d1d1a-f4b5-41ed-871e-1ab9bdfa39ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentences(examples):\n",
    "    return {'sentence': ' '.join(examples[\"token\"])}\n",
    "\n",
    "re_dataset = re_dataset.map(add_sentences)\n",
    "re_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28c745-b2d2-4c0e-96d8-67fa828da2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./../../../hf_models/distilbert-base-uncased/\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa3793-8b1e-41c4-a75c-ea1531bfbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenized_dataset = re_dataset.map(tokenize, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce5a8d-d2c6-427f-bb04-84d5805d3a67",
   "metadata": {},
   "source": [
    "**Parallelization** can be achieved using the parameter `batched=True`, thus making the process fast. For large datasets, multiprocessing can be enabled using the parameter `num_proc` to specify the number of processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa82e1-c86e-40f1-bb99-0a3e8a498e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111695bb-34b5-4487-aaae-d1bce515e530",
   "metadata": {},
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb910d-6611-41e6-936d-a91040fd9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81581e9-441d-45b0-9c41-cee85355f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_sc_dataset = sc_dataset[\"train\"].train_test_split(train_size=0.9, seed=25)\n",
    "splitted_sc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff9cde-2962-4c21-95c8-baca63cd6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_sc_dataset[\"dev\"] = splitted_sc_dataset.pop(\"test\")\n",
    "splitted_sc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f8a74-909e-4e69-8e56-ebef509d0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sc_dataset = splitted_sc_dataset[\"train\"].train_test_split(train_size=0.9, seed=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e02f39-8d1b-4502-8418-5b9207a1a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sc_dataset[\"dev\"] = splitted_sc_dataset[\"dev\"]\n",
    "final_sc_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250034b-6e26-48ed-be20-ea66431ba573",
   "metadata": {},
   "source": [
    "## Saving the dataset\n",
    "\n",
    "The dault format is *Arrow*. Using default function `save_to_disk()`, the dataset will be saved in *Arrow*, where each split is associated with its own *dataset.arrow* table, and some metadata in *dataset_info.json* and *state.json*. \n",
    "\n",
    "|Data format| \tFunction|\n",
    "|-----------|-----------|\n",
    "|Arrow| \tDataset.save_to_disk()|\n",
    "|CSV| \tDataset.to_csv()|\n",
    "|JSON| \tDataset.to_json()|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a081e0-8c8b-45f9-9581-f8ace4210083",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in final_sc_dataset.items():\n",
    "    dataset.to_csv(f\"./../../data/FiQA_and_Financial_PhraseBank_in_1/{split}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4726a9-875a-40d2-9046-09e95edff916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

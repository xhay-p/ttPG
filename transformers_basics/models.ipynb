{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbf30dc-1e1c-4594-b3b6-22f86c092009",
   "metadata": {},
   "source": [
    "## Loading the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83d4911-9ec6-4ffb-978a-e3847a722054",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"LTIMindtree Q2FY24: Show of strength. Good revenue growth and resilient margin performance\",\n",
    "        \"The company expects furloughs to be more pronounced in Q3 and it is guiding to a very weak quarter, with revenue decline between 1.5 percent and 3.5 percent\",\n",
    "        \"Arkam Ventures is also an investor in Jai Kisan, one of India’s fastest-growing rural fintech platforms for farmers and retailers, and Jumbotail, India’s leading B2B food and grocery marketplace and retail platform\",\n",
    "       ]\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "input = tokenizer(data, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52188483-cbae-4ac9-8536-46bfcd3cf82d",
   "metadata": {},
   "source": [
    "## AutoModel Class\n",
    "\n",
    "Class `AutoModel` is generally used to instantiate any model from a checkpoint. However, to load a model for a specific task, there are several variants of `AutoModel` class is defined in the `transformers` library. Some of them are:\n",
    "* `AutoModelForCausalLM`\n",
    "* `AutoModelForQuestionAnswering`\n",
    "* `AutoModelForSequenceClassification`\n",
    "* `AutoModelForTokenClassification`, and others.\n",
    "\n",
    "`AutoModel` is generally designed to retrieve the hidden states, whereas the other ones are designed for a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d860e6ae-1300-4097-96c0-7f924632cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f681eef3-b640-49d2-8f4c-7f036fc62c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecbab193-a1b7-4530-8e75-ef745e4c1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = classifier(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ee62ea-1f60-4c0e-9b6c-f087693cd71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.5171, -0.2222,  0.3759,  ...,  0.2316,  0.8865, -0.5954],\n",
       "         [ 0.2151,  0.0263,  0.9891,  ...,  0.0113,  0.1795,  0.3608],\n",
       "         [ 0.1851,  0.0462,  0.7588,  ..., -0.4426,  0.4743,  0.0501],\n",
       "         ...,\n",
       "         [ 0.6706, -0.2922,  0.0375,  ...,  0.3897,  0.7996, -0.5716],\n",
       "         [ 0.3803, -0.2894,  0.2847,  ...,  0.7460,  0.8029, -0.3096],\n",
       "         [ 0.5641, -0.6835,  0.2739,  ...,  0.7024,  0.5494, -0.4684]],\n",
       "\n",
       "        [[-1.2077,  0.3169,  0.1434,  ..., -0.0836, -0.4359, -0.3140],\n",
       "         [-1.1384,  0.4225, -0.1258,  ...,  0.0325, -0.0875, -0.3803],\n",
       "         [-0.6221,  0.7449, -0.1054,  ..., -0.3525,  0.0835,  0.0054],\n",
       "         ...,\n",
       "         [-1.0930,  0.3044,  0.2513,  ..., -0.1011, -0.4593, -0.2472],\n",
       "         [-1.0747,  0.3590,  0.2335,  ..., -0.0922, -0.4178, -0.2995],\n",
       "         [-1.1595,  0.3961,  0.0261,  ..., -0.2430, -0.3581, -0.2027]],\n",
       "\n",
       "        [[ 0.1207, -0.0831,  0.2845,  ..., -0.0777,  0.6180, -0.7339],\n",
       "         [-0.5340,  1.0797,  0.9417,  ...,  0.0458,  0.7938, -0.3334],\n",
       "         [-0.1519,  0.6214,  0.5919,  ...,  0.1682,  0.7219, -0.7844],\n",
       "         ...,\n",
       "         [ 0.9066,  0.0441,  0.9975,  ..., -0.0950,  0.4207, -0.9434],\n",
       "         [ 0.1887,  0.2791,  0.7419,  ...,  0.1313,  0.4891, -0.8005],\n",
       "         [ 1.1615, -0.0610,  0.4592,  ...,  0.4739, -0.0198, -1.0139]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b501c7bc-3122-4b81-b93f-f7c26a5361bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af16da-a515-4bb0-b961-42a86cd6ab54",
   "metadata": {},
   "source": [
    "**AutoModel** is generally used for retrieving the hidden state which can be used as feature. \n",
    "\n",
    "For Sentiment Analysis, it is better to use **AutoModelForSequenceClassification**. This comes with Sequence Classification heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e96f177-5aaf-4cbd-9a7b-6755709d0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "046e4476-c444-4495-b77a-31a8e97db41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-3.7354,  3.9795],\n",
       "        [ 4.2851, -3.5077],\n",
       "        [-3.0048,  3.1662]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183eaf2-5dff-4df8-8a42-0304a56983cb",
   "metadata": {},
   "source": [
    "Returned logits are specific to `Binary Classification`. For every example two values are given. One for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a12161-5acb-4e27-aafb-839e1b056333",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a95e1e-81ba-40c9-a9d1-b37e6330ba9e",
   "metadata": {},
   "source": [
    "### Loading the model for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4823c59-95d1-437c-9098-17a3721acd13",
   "metadata": {},
   "source": [
    "#### Loading the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82843e03-d2e7-4b02-af3f-e3134771910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01852ac6-99b3-44d1-82a5-fee3abda9931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"activation\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2103e-ae81-4df7-91aa-75c399352ce6",
   "metadata": {},
   "source": [
    "The configurtions contains parameters used to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33543526-faf3-4a93-94ce-0407f4a3c1b5",
   "metadata": {},
   "source": [
    "#### Loading the model using the config\n",
    "\n",
    "This loads the model with random parameter initialization. This can further be used for training the model specific to any task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e42ddbe8-fade-4259-b5fc-85744b5fdc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "084ac847-b10c-4353-ad5b-2c44a92bb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67177458-183e-45fd-9700-bc060caf2171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbf4fd-1f8f-4586-b4bd-a34ab3f7e98b",
   "metadata": {},
   "source": [
    "### Loading the model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe070fd-04a1-44db-9d85-e49ad2d64f6b",
   "metadata": {},
   "source": [
    "For inference the model requires pre-trained weights (checkpoint). Either one can train the model on their own dataset and load from that checkpoint or can use the one available on huggingface hub using `from_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e5a7201-c298-4298-b0da-92b175fac1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b1889-47e3-415c-b8fe-b5bcd41219ca",
   "metadata": {},
   "source": [
    "This will download the model to a cache dir `~/.cache/huggingface/transformers`. To download the model to a specific directory use the parameter `cache_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e9a8ff3-be50-4486-ae36-f6164b5d9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "tokenizer =  AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e794736a-68ef-4b93-99ce-8aab9d891e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 4.4096e-04, -2.6241e-01, -1.0192e-01,  ..., -6.2764e-02,\n",
       "           2.7584e-01,  3.7014e-01],\n",
       "         [ 7.2233e-01,  1.6449e-01,  4.0025e-01,  ...,  1.9161e-01,\n",
       "           4.0458e-01, -5.8094e-02],\n",
       "         [ 2.8198e-01, -1.7430e-01,  3.9076e-02,  ...,  2.7681e-02,\n",
       "           1.1886e-01,  9.1439e-01],\n",
       "         ...,\n",
       "         [ 6.8016e-01,  7.9712e-02,  8.3603e-01,  ..., -4.8959e-01,\n",
       "          -2.5017e-01, -2.3519e-01],\n",
       "         [ 3.8105e-02, -8.1751e-01, -3.4076e-01,  ...,  4.4815e-01,\n",
       "           9.6725e-02, -2.0311e-01],\n",
       "         [ 3.5750e-01,  1.9968e-01,  1.7437e-01,  ...,  1.5028e-01,\n",
       "          -2.3665e-01,  5.4391e-02]]], grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffa3b6b-9f76-4e77-b75b-73b01977ef1c",
   "metadata": {},
   "source": [
    "#### Using the model in pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ccfae1b-a198-4761-a847-90450a6d4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, DistilBertForMaskedLM\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a700017-4742-4fa1-a92b-74b43806a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline(task='fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2961a3f8-051a-40e2-9282-40c3430fc7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.04683324694633484,\n",
       "  'token': 2711,\n",
       "  'token_str': 'person',\n",
       "  'sequence': \"replace me by any person you'd like.\"},\n",
       " {'score': 0.03321794793009758,\n",
       "  'token': 2171,\n",
       "  'token_str': 'name',\n",
       "  'sequence': \"replace me by any name you'd like.\"},\n",
       " {'score': 0.023977328091859818,\n",
       "  'token': 2450,\n",
       "  'token_str': 'woman',\n",
       "  'sequence': \"replace me by any woman you'd like.\"},\n",
       " {'score': 0.021561430767178535,\n",
       "  'token': 8016,\n",
       "  'token_str': 'excuse',\n",
       "  'sequence': \"replace me by any excuse you'd like.\"},\n",
       " {'score': 0.017540020868182182,\n",
       "  'token': 2158,\n",
       "  'token_str': 'man',\n",
       "  'sequence': \"replace me by any man you'd like.\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Replace me by any [MASK] you'd like.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865df0c-af6a-45a5-8812-9b060060a232",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "Here, saving the checkpoint downloaded from the hub. One can save the model similarily after training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20add674-7916-40e5-8289-ebf6f867456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./../../../hf_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f9264-43c5-4298-aecf-5ad4f473d357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
